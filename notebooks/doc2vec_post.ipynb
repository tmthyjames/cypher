{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__FRONT_MATTER__\n",
    "\n",
    "---\n",
    "title: \"Analyzing Rap Lyrics Using Word Vectors\"\n",
    "author_profile: true\n",
    "author: Tim Dobbins\n",
    "excerpt: In this post, we'll analyze lyrics from the best rappers of all time.\n",
    "categories: [posts]\n",
    "tags: [python, machine-learning, data-science]\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__CUSTOM_CSS__\n",
    "table {\n",
    "    white-space:nowrap;\n",
    "}\n",
    "div.output_area {\n",
    "    max-height: 400px;\n",
    "}\n",
    "div.input_area {\n",
    "    margin: 5px;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we'll analyze lyrics from the best rappers of all time. To do this, we'll use Python and [gensim's](https://radimrehurek.com/gensim/) implementation of the [Doc2Vec](https://arxiv.org/abs/1405.4053) algortithm. To get the data, we'll use [Cypher](https://github.com/tmthyjames/cypher), a new Python package [I recently released](https://tmthyjames.github.io/tools/Cypher/) that retrieves music lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "• [Doc2Vec Summary](#Quick-Note-on-Doc2Vec)<br/>\n",
    "• [Getting the Data](#Getting-the-Data)<br/>\n",
    "• [Loading the Data](#Loading-the-Data)<br/>\n",
    "• [Initializing the Model](#Initializing-the-Model)<br/>\n",
    "• [Training the Model](#Training-the-Model)<br/>\n",
    "• [Finding Most Similar Words](#Finding-Most-Similar-Words)<br/>\n",
    "• [Finding Most Similar Documents](#Finding-Most-Similar-Documents)<br/>\n",
    "• [Infering Vectors](#Infering-Vectors)<br/>\n",
    "• [Up Next](#Up-Next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Note on Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is an extension of Word2Vec, an algorithm that employs a shallow neural network to map words to a vector space called word vectors (or word embeddings). Whereas Word2Vec produces word vectors so you can run similarity queries between <i>words</i>, Doc2Vec produces document vectors so you can run similarity queries on whole sentences, paragraphs, or documents. Finding semantic similarities is based on the distributional hypothesis that states words that appear in the same contexts share the same meaning. Or, as the English linguist J. R. Firth put it, \"a word is characterized by the company it keeps\".\n",
    "\n",
    "My aim for this post isn't to cover the theory or math behind Doc2Vec but to show its power. For a deeper overview of Doc2Vec, see [here](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get all the lyrics for the top 100 rappers, we'll use [Cypher](https://github.com/tmthyjames/cypher), a new python library [I released](https://tmthyjames.github.io/tools/Cypher/) recently to retrieve music lyrics (to install: `pip install thecypher`). But first, we need to get a list of the top 100 rappers. For this, I just Googled \"top rappers\" and got a hit from [ranker.com](https://www.ranker.com/crowdranked-list/the-greatest-rappers-of-all-time). This will suffice, although I don't think this list is perfect. Luckily they source the data from an API so we don't have to screen scrape! Here's the code to get this list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tupac', 'Eminem', 'The Notorious B.I.G.', 'Nas', 'Ice Cube', 'Jay-Z', 'Snoop Dogg', 'Dr. Dre', 'Kendrick Lamar', 'Rakim', 'André 3000', 'Eazy-E', 'Kanye West', '50 Cent', 'DMX', 'Busta Rhymes', 'Method Man', 'J. Cole', 'Mos Def', 'Ludacris', 'KRS-One', 'LL Cool J', 'Lil Wayne', 'Common', 'Big L', 'Ghostface Killah', 'Redman', 'T.I.', 'Big Pun', 'Nate Dogg', 'Tech N9ne', 'Lauryn Hill', 'Scarface', 'Slick Rick', 'Raekwon', 'Big Daddy Kane', \"Ol' Dirty Bastard\", 'The Game', 'Mobb Deep', 'Logic', 'Chance the Rapper', 'Cypress Hill', 'Ice-T', 'Lupe Fiasco', 'RZA', 'GZA', 'Q-Tip', 'Warren G', 'Talib Kweli', 'Xzibit', 'Missy Elliott', 'ASAP Rocky', 'Joey Badass', 'Immortal Technique', 'Twista', 'Big Sean', 'Kid Cudi', 'Big Boi', 'Chuck D', 'Donald Glover', 'Drake', 'Wiz Khalifa', 'Eric B. & Rakim', 'Schoolboy Q', 'DMC', 'Nelly', 'Hopsin', 'D12', 'Jadakiss', 'Tyler, the Creator', 'Kurupt', 'Grandmaster Flash and the Furious Five', 'Gang Starr', 'Too $hort', 'Royce da 5&#39;9&#34;', 'MC Ren', 'E-40', 'Pusha T', 'Coolio', 'De La Soul', 'Proof', 'Bad Meets Evil', 'Guru', 'Will Smith', 'Krayzie Bone', 'Black Thought', 'B.o.B', 'AZ', 'Yelawolf', 'The Sugarhill Gang', 'Earl Sweatshirt', 'Fabolous', 'Mac Miller', 'Fat Joe', 'Young Jeezy', 'Kool G Rap', 'Bizzy Bone', 'Queen Latifah', 'Prodigy', '2 Chainz']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://cache-api.ranker.com/lists/855723/items'\\\n",
    "      '?limit=100&offset=0&include=votes,wikiText,rankings,'\\\n",
    "      'openListItemContributors&propertyFetchType=ALL&liCacheKey=null'\n",
    "r = requests.get(url)\n",
    "data = r.json()\n",
    "artists = [i['name'] for i in data['listItems']]\n",
    "\n",
    "print(artists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Cypher to retrieve these lyrics we'll loop over the list and run `thecypher.get_lyrics` on each artist. The following will `get_lyrics` and then convert it to a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>id</th>\n",
       "      <th>lyric</th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Infinite (1996)</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Hip_Hop</td>\n",
       "      <td>14201</td>\n",
       "      <td>Oh yeah, this is Eminem baby, back up in that motherfucking ass</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Infinite (1996)</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Hip_Hop</td>\n",
       "      <td>14202</td>\n",
       "      <td>One time for your mother fucking mind, we represent the 313</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Infinite (1996)</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Hip_Hop</td>\n",
       "      <td>14203</td>\n",
       "      <td>You know what I'm saying?, 'cause they don't know shit about this</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infinite (1996)</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Hip_Hop</td>\n",
       "      <td>14204</td>\n",
       "      <td>For the 9-6</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Infinite (1996)</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Hip_Hop</td>\n",
       "      <td>14205</td>\n",
       "      <td>Ayo, my pen and paper cause a chain reaction</td>\n",
       "      <td>Infinite</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             album  artist    genre     id  \\\n",
       "0  Infinite (1996)  Eminem  Hip_Hop  14201   \n",
       "1  Infinite (1996)  Eminem  Hip_Hop  14202   \n",
       "2  Infinite (1996)  Eminem  Hip_Hop  14203   \n",
       "3  Infinite (1996)  Eminem  Hip_Hop  14204   \n",
       "4  Infinite (1996)  Eminem  Hip_Hop  14205   \n",
       "\n",
       "                                                               lyric  \\\n",
       "0  Oh yeah, this is Eminem baby, back up in that motherfucking ass     \n",
       "1  One time for your mother fucking mind, we represent the 313         \n",
       "2  You know what I'm saying?, 'cause they don't know shit about this   \n",
       "3  For the 9-6                                                         \n",
       "4  Ayo, my pen and paper cause a chain reaction                        \n",
       "\n",
       "       song  year  \n",
       "0  Infinite  1996  \n",
       "1  Infinite  1996  \n",
       "2  Infinite  1996  \n",
       "3  Infinite  1996  \n",
       "4  Infinite  1996  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import thecypher\n",
    "import pandas as pd\n",
    "\n",
    "lyrics = []\n",
    "for artist in artists:\n",
    "    \n",
    "    # our Cypher code\n",
    "    artist_lyrics = thecypher.get_lyrics(artist)\n",
    "    \n",
    "    # append each record\n",
    "    [lyrics.append(i) for i in artist_lyrics]\n",
    "\n",
    "# convert to a DataFrame\n",
    "lyrics_df = pd.DataFrame(lyrics)\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the data is delivered with one lyric per row. The following code will convert it to one song per row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>album</th>\n",
       "      <th>genre</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>1996</td>\n",
       "      <td>Infinite (1996)</td>\n",
       "      <td>Hip_Hop</td>\n",
       "      <td>Eminem</td>\n",
       "      <td>Eye-Kyu: Now what you know about a sweet MC, from the 313 None of these skills you bout to see come free So you wanna be a sweet MC, you gotta become me If you ever wanna be one see Eminem: Man what you know about a sweet MC, in the 313 None of these skills you bout to see come free So you wanna be a sweet MC, you better become me If you ever wanna be one see Verse 1: Eye-Kyu Yo some people say I'm whack, now if that's right I'm the freshest whack MC that you ever heard, in your lifetime My slick accapella sounds clever with the beats Boy I'm the deepest thing since potholes to ever hit the streets Forgot a gold digger's succubus, my souls thick with ruggedness With the mic I'm like a dyke, can't no nigga fuck with this I got more Different Strokes than Philip Drummound On open mic I bone your women just to keep my lyrics coming (bitch) We elevated to new heights premeditated Let it be that I stated they hate it now that they see that I made it The escalated can be put to the test of greatness Snatch the heart from MC's and I ate it So I take it that's the reason I'm hated To represent my temperament If rap was a dick all you so called hard MC's would not be impitant But pimping it, and acting like you could rock a show (so) Harder than LL's Rock the Bells, but you is a ho (now) Everything that you collaborate I lacerate My rhymes they keep coming like nympho maniacs that masturbate At a faster rate, yeah I got something for your ass to hate I blasterate, and have you all running master gates And as for face clutching and touching the flows I got them open like marijuana smoke up in your nose Bucking these hoes, I got that shit down to a science Leaving them hot and bothered, turned on like an appliance Defiance, no we won't have that You want your shit to blow up? Well I'ma stuff some dynamite in your ass crack And blast that shit to kingdom come Then bring them some of this real hip-hop I drop beats and you ain't singing or gonna do a thing about And you all knew from Meeko That you couldn't hold your own with the strength of Lou Forigno So stop that bullshit and flow Yo, you need to come with the real skills, and act like you know Chorus: Eminem So what you know about a sweet MC, in the 313 None of these skills you bout to see come free So you wanna be a sweet MC, you better become me If you ever wanna be one see Eye-Kyu: Now what you know about a sweet MC, from the 313 None of these skills you bout to see come free So you wanna be the sweet MC, you gotta become me If you ever wanna be one see Verse 2: Eminem So what, you know about a sweet MC, in the 313 You don't know shit so when you see one flee You can be Run-D, you'll never beat the MC I'll stop the alphabet at S and got it down to a T I'm sure your bound to agree, a sweet MC crashes the spot I'll make the roof hot like I was Rock Master Scott Your ass forgot, so just in case you don't remember me I'll run your brain around the block to jog your fucking memory It's either them or me man, kill or be killed You will and be sealed your casket closed you still gonna be billed My facilities filled with fans, packed to capacity I'll send a rapper back with the crack of his ass shitty If he's acting soft and he cowers He better come cleaner then Jay Rue jacking off when he showers You flowers got no clout with a thing You could date a stick of dynamite and wouldn't go out with a bang I showered the slang, simple as A, B, C's Skip over the D's and rock the microphone with E's Dethrone MC's and I'ma max alone Relax your dome like a solo from a saxophone So facts are known, writers get treated with shocks I rock a beat harder then you could beat it with rocks I'm greeted with flocks, of fellow follower's singers You couldn't make the fans throw up their hands if they swallowed their Fingers But you can bring yours let's see what you got But don't front and never try to be what you're not Cause you can be quick, jump the candlestick, burn your back And fuck Jill on a hill, but you still ain't Jack Chorus: Eye-Kyu So what you know about a sweet MC, from the 313 None of these skills you just seen come free So you wanna be a sweet MC, you'll never become me So you ain't ever gonna be one see Eminem: So what you know about a sweet MC, in the 313 None of these skills that you just seen come free So you wanna be a sweet MC, you'll never become me So you ain't ever gonna be one see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  song  year            album    genre  artist  \\\n",
       "0  313  1996  Infinite (1996)  Hip_Hop  Eminem   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  lyric  \n",
       "0  Eye-Kyu: Now what you know about a sweet MC, from the 313 None of these skills you bout to see come free So you wanna be a sweet MC, you gotta become me If you ever wanna be one see Eminem: Man what you know about a sweet MC, in the 313 None of these skills you bout to see come free So you wanna be a sweet MC, you better become me If you ever wanna be one see Verse 1: Eye-Kyu Yo some people say I'm whack, now if that's right I'm the freshest whack MC that you ever heard, in your lifetime My slick accapella sounds clever with the beats Boy I'm the deepest thing since potholes to ever hit the streets Forgot a gold digger's succubus, my souls thick with ruggedness With the mic I'm like a dyke, can't no nigga fuck with this I got more Different Strokes than Philip Drummound On open mic I bone your women just to keep my lyrics coming (bitch) We elevated to new heights premeditated Let it be that I stated they hate it now that they see that I made it The escalated can be put to the test of greatness Snatch the heart from MC's and I ate it So I take it that's the reason I'm hated To represent my temperament If rap was a dick all you so called hard MC's would not be impitant But pimping it, and acting like you could rock a show (so) Harder than LL's Rock the Bells, but you is a ho (now) Everything that you collaborate I lacerate My rhymes they keep coming like nympho maniacs that masturbate At a faster rate, yeah I got something for your ass to hate I blasterate, and have you all running master gates And as for face clutching and touching the flows I got them open like marijuana smoke up in your nose Bucking these hoes, I got that shit down to a science Leaving them hot and bothered, turned on like an appliance Defiance, no we won't have that You want your shit to blow up? Well I'ma stuff some dynamite in your ass crack And blast that shit to kingdom come Then bring them some of this real hip-hop I drop beats and you ain't singing or gonna do a thing about And you all knew from Meeko That you couldn't hold your own with the strength of Lou Forigno So stop that bullshit and flow Yo, you need to come with the real skills, and act like you know Chorus: Eminem So what you know about a sweet MC, in the 313 None of these skills you bout to see come free So you wanna be a sweet MC, you better become me If you ever wanna be one see Eye-Kyu: Now what you know about a sweet MC, from the 313 None of these skills you bout to see come free So you wanna be the sweet MC, you gotta become me If you ever wanna be one see Verse 2: Eminem So what, you know about a sweet MC, in the 313 You don't know shit so when you see one flee You can be Run-D, you'll never beat the MC I'll stop the alphabet at S and got it down to a T I'm sure your bound to agree, a sweet MC crashes the spot I'll make the roof hot like I was Rock Master Scott Your ass forgot, so just in case you don't remember me I'll run your brain around the block to jog your fucking memory It's either them or me man, kill or be killed You will and be sealed your casket closed you still gonna be billed My facilities filled with fans, packed to capacity I'll send a rapper back with the crack of his ass shitty If he's acting soft and he cowers He better come cleaner then Jay Rue jacking off when he showers You flowers got no clout with a thing You could date a stick of dynamite and wouldn't go out with a bang I showered the slang, simple as A, B, C's Skip over the D's and rock the microphone with E's Dethrone MC's and I'ma max alone Relax your dome like a solo from a saxophone So facts are known, writers get treated with shocks I rock a beat harder then you could beat it with rocks I'm greeted with flocks, of fellow follower's singers You couldn't make the fans throw up their hands if they swallowed their Fingers But you can bring yours let's see what you got But don't front and never try to be what you're not Cause you can be quick, jump the candlestick, burn your back And fuck Jill on a hill, but you still ain't Jack Chorus: Eye-Kyu So what you know about a sweet MC, from the 313 None of these skills you just seen come free So you wanna be a sweet MC, you'll never become me So you ain't ever gonna be one see Eminem: So what you know about a sweet MC, in the 313 None of these skills that you just seen come free So you wanna be a sweet MC, you'll never become me So you ain't ever gonna be one see  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = ['song', 'year', 'album', 'genre', 'artist']\n",
    "lyrics_by_song = lyrics_df.sort_values(group)\\\n",
    "       .groupby(group).lyric\\\n",
    "       .apply(' '.join)\\\n",
    "       .reset_index(name='lyric')\n",
    "    \n",
    "lyrics_by_song.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the data [here](https://github.com/tmthyjames/cypher/tree/master/data), randomly split into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the data. Doc2Vec requires A LOT of memory, so we'll create an iterator so our data doesn't have to be loaded into memory simultaneously. Instead, we load one document at a time, train the model on it, then discard it and move on to the next document. We could also stream this data from a database if we wanted. Here's how you stream the data from a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "class Sentences(object):\n",
    "    \n",
    "    def __init__(self, filename, column):\n",
    "        self.filename = filename\n",
    "        self.column = column\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_tokens(text):\n",
    "        \"\"\"Helper function for tokenizing data\"\"\"\n",
    "        return [wnl.lemmatize(r.lower()) for r in text.split()]\n",
    " \n",
    "    def __iter__(self):\n",
    "        reader = csv.DictReader(open(self.filename, 'r' ))\n",
    "        for row in reader:\n",
    "            words = self.get_tokens(row[self.column])\n",
    "            tags = ['%s|%s' % (row['artist'], row['song_id'])]\n",
    "            yield TaggedDocument(words=words, tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple things to note. First, the Doc2Vec model accepts a list of `TaggedDocument` elements which will allow us to identify a song. Second, we use `wnl.lemmatize` as apart of our tokenization so we can group together the inflected forms of a word so they can be analysed as a single word. For instance, `wnl.lemmatize` will convert 'cars' into 'car'.\n",
    "\n",
    "\n",
    "To initialize our Sentence object, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'lyrics_train.csv'\n",
    "sentences = Sentences(filename=filename, column='word')\n",
    "\n",
    "# for song lookups\n",
    "df_train = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize our `Doc2Vec` model, we'll do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "model = Doc2Vec(\n",
    "    alpha=0.025,\n",
    "    min_alpha=0.025,\n",
    "    workers=15, \n",
    "    min_count=2,\n",
    "    window=10,\n",
    "    size=300,\n",
    "    iter=20,\n",
    "    sample=0.001,\n",
    "    negative=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over each argument.\n",
    "\n",
    "• `alpha` is the initial learning rate. A very intuitive explanation for learning rate can be found [here](https://www.quora.com/What-is-the-learning-rate-in-neural-networks). Essentially, the learning rate is, as stated in the link, \"how quickly a network abandons old beliefs for new ones.\" <br/>\n",
    "• `min_alpha` is exactly what it sounds like, the minimum `alpha` can be, which we reduce after every epoch. <br/>\n",
    "• `workers` is the number of threads used to train the model. <br/>\n",
    "• `min_count` specifies a term frequency that must be met for a word to be considered by the model. <br/>\n",
    "• `window` is how many words in front and behind the input word should be considered when determining context. <br/>\n",
    "• `size` is the number of dimensions. Unlike most numerical datasets that have 2 dimensions, text data can have hundreds or even more.<br/>\n",
    "• `iter` is the number of iterations, the number of times the training set passes through the algorithm. <br/>\n",
    "• `sample` is the downsampling rate. Words representing more than this will be eligible for downsampling.<br/>\n",
    "• `negative` is the negative sampling rate. 0 means update all weights in the output layer of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our vocabulary and train our model. We'll train our model for 10 epochs. To understand epochs and how they differ from iterations (from above), check out [this](https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks) StackOverflow post. Namely this answer:\n",
    "\n",
    "> In the neural network terminology:\n",
    "\n",
    "> one epoch = one forward pass and one backward pass of all the training examples\n",
    "\n",
    "> batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
    "\n",
    "> number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
    "\n",
    "We use multiple epochs because neural networks typically require an iterative optimization method to produce good results, which usually means several passes over the data.\n",
    "\n",
    "After each epoch, we'll decrease the learning rate (known as learning rate decay). This is to help speed up our training. For more on learning rate decay and the intuition behind it, see Andrew Ng's [video](https://www.coursera.org/learn/deep-neural-network/lecture/hjgIA/learning-rate-decay) on the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.build_vocab(sentences)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "    model.alpha -= 0.002  # decrease the learning rate\n",
    "    model.min_alpha = model.alpha  # fix the learning rate, no decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To persist our model so we can use it later without training it again, we'll use `model.save` and load it using `Doc2Vec.load` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('rap-lyrics.doc2vec')\n",
    "\n",
    "model = Doc2Vec.load('rap-lyrics.doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll find the most similar words given a target word. Similar words, in this context, refers to words that have similar vector representations. Let's first see what one of these vector representations looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08730847, -0.75961363,  1.38362062, -0.6143629 ,  0.38046223,\n",
       "       -0.27822378,  1.0065887 ,  0.66717136,  0.53995496, -0.23645727,\n",
       "       -0.54589874,  0.0852062 , -1.74815035,  0.11079719, -0.08960737,\n",
       "        0.529109  , -0.50958592, -0.17503066, -0.79260975,  0.14438754,\n",
       "        0.77649647, -0.45132214,  0.26107937, -0.94072151,  0.33201343,\n",
       "        0.06891677,  0.07961012,  0.4604567 ,  0.59327006, -0.97538424,\n",
       "        0.72243172, -0.62705523, -0.67403787, -0.49406284, -0.12099945,\n",
       "        0.94990158, -0.13507502, -0.28207451,  0.26398847, -1.06900597,\n",
       "       -0.00755116,  0.57757616,  1.11100399, -1.2982794 , -0.49452487,\n",
       "       -0.87145579,  0.95555776, -0.11877067, -0.43198681, -0.93733525,\n",
       "        0.37859944, -0.30048838, -0.66467839,  0.18476482,  1.00505781,\n",
       "       -0.32252848,  0.37282225, -0.25394279, -1.34661531, -0.52854782,\n",
       "        1.13223743,  0.99049121,  0.46284243, -0.1918252 ,  0.13938105,\n",
       "       -0.48491701,  0.51925433,  1.20754588, -0.96833384,  0.79104269,\n",
       "       -0.73094076,  0.47804666, -0.83540857,  0.28851396,  0.63589162,\n",
       "       -0.0701448 ,  0.60138482,  0.44773316,  0.00519398, -0.86304641,\n",
       "        0.60134429, -0.47746554, -0.29469249, -0.53207111, -0.01317888,\n",
       "        0.25100547, -0.28447381,  0.30167863, -0.69652796,  0.06100113,\n",
       "        0.31017253, -0.81000185,  1.12655675,  0.43748701, -0.22233665,\n",
       "       -0.10591302, -0.20906276,  0.64186734,  0.75487345, -0.33429062,\n",
       "        0.61145753, -0.60427082,  1.01324248,  0.41736758,  0.61233884,\n",
       "       -0.45689461,  0.0348687 , -1.13389814, -0.56958205, -0.07068884,\n",
       "        0.38049203, -0.16118132, -0.24486333, -0.15606797, -0.39647394,\n",
       "       -0.83589935,  1.02677846,  1.3442173 , -1.14568281,  0.83720064,\n",
       "        0.16027951, -0.12637103, -0.19827659, -0.66922218, -0.29960367,\n",
       "       -0.05307895, -0.37429324,  0.26029611,  0.06835603, -0.17044738,\n",
       "       -0.16344468, -0.11831434, -1.00676751,  0.03664918,  0.09502432,\n",
       "       -0.02389756, -0.19385858,  0.30843341, -0.17243858, -0.58546185,\n",
       "        0.55559689, -1.72106338,  1.57498956,  0.84389043,  1.45231485,\n",
       "        0.57232487,  0.19501007,  0.63717937,  1.0293175 , -0.92565066,\n",
       "        0.27495563,  0.99241298, -0.36239591, -0.63944608,  0.98583406,\n",
       "        0.28459859,  0.16633834, -0.25373855,  1.0375272 ,  0.87758309,\n",
       "       -1.28847861,  1.21945608, -0.6350345 , -0.04999185, -0.00272338,\n",
       "       -1.08706963,  0.87849152, -0.94101387,  0.19403276, -0.36275229,\n",
       "        0.59413522,  0.07580746, -1.02804196,  0.55732048,  0.39909068,\n",
       "       -0.33037657,  0.21233572,  0.47778675, -0.79651719,  1.00793493,\n",
       "       -0.41612679, -0.53737336,  0.24760191,  0.38230151,  0.73036176,\n",
       "       -0.35786813, -1.58644748,  1.13606143, -0.74370193, -0.42779964,\n",
       "        0.50492698,  0.09908741,  1.68453491, -0.58360714, -0.00237251,\n",
       "        0.35537153, -1.02333701, -0.34471443,  0.63144279, -0.46631771,\n",
       "        0.38779327,  0.50322723,  0.38161048, -0.24549502, -0.61341262,\n",
       "       -1.10746717, -0.32898605,  0.24342866, -0.35774669, -0.33637381,\n",
       "        0.80286038, -1.16234457,  0.46797028,  0.73834223,  1.06542349,\n",
       "       -0.23826197, -0.06502097,  0.99523503, -0.62745804,  0.15714839,\n",
       "       -0.33410352,  0.58607596,  0.94445592, -0.66240662, -0.31714687,\n",
       "       -0.46530724,  0.47603473,  0.86875081, -0.08624794, -1.44019818,\n",
       "        0.66523236,  0.58958417, -0.83577442,  0.74504346, -0.54804331,\n",
       "       -0.35254389, -0.48228839, -0.74581242, -0.44964668, -0.37321514,\n",
       "        0.66511476, -0.81892806, -0.73356837,  0.7078895 ,  0.16557175,\n",
       "        0.4391076 , -0.1286169 ,  0.58950937, -0.48881784,  0.8607918 ,\n",
       "       -0.39296728, -0.28481975, -0.04146343,  0.0630324 , -0.08137196,\n",
       "       -0.42649627, -0.28637046, -0.80750531, -0.26168498, -0.09741032,\n",
       "       -1.18555474,  1.20315957, -0.88750184, -0.29315889, -1.05850375,\n",
       "       -0.81591237,  0.20775978, -1.01604664, -0.11606085,  1.0934279 ,\n",
       "        0.87924618,  0.29393262,  0.29348886,  0.31612197,  0.01333179,\n",
       "        0.26205558, -0.48351637,  0.34280518, -0.75954032, -0.03328871,\n",
       "       -0.28338876, -0.00927117, -0.29307657, -0.54590905,  0.82289994,\n",
       "       -0.79231226, -0.39733326, -1.24357569,  0.08877811, -0.65606076,\n",
       "        0.0533943 , -0.49978754,  0.14410768, -0.15699422, -1.22454178,\n",
       "       -0.40445986, -1.68313813, -1.52885747, -0.621813  , -0.09528219], dtype=float32)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.word_vec('rap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Most Similar Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results produced by Doc2vec are very impressive. To showcase, we'll start with the `most_similar` method which finds the top n words most similar to the target word. We can see from the following that the results are accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crib', 0.4296485483646393),\n",
       " ('room', 0.33615612983703613),\n",
       " ('club', 0.30419921875),\n",
       " ('place', 0.29620522260665894),\n",
       " ('mansion', 0.2891782522201538),\n",
       " ('spot', 0.2849082350730896),\n",
       " ('garage', 0.28439778089523315),\n",
       " ('town', 0.2630491256713867),\n",
       " ('south', 0.2609255313873291),\n",
       " ('trunk', 0.26089051365852356)]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('house')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tree', 0.45602014660835266),\n",
       " ('chronic', 0.3657829761505127),\n",
       " ('bud', 0.34473711252212524),\n",
       " ('reefer', 0.33160412311553955),\n",
       " ('blantz', 0.32347556948661804),\n",
       " ('dope', 0.3029516637325287),\n",
       " ('blunts', 0.2944639325141907),\n",
       " ('blunt', 0.2931532859802246),\n",
       " ('hahahahahaaa', 0.2876523733139038),\n",
       " ('drug', 0.2835467457771301)]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('weed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found this next result to be very interesting. There apparently is a double meaning to the word 'seed' and our model captures both meanings, an offspring and another word for weed. That's cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('child', 0.30444782972335815),\n",
       " ('greed', 0.2916702926158905),\n",
       " ('leaf', 0.2634624242782593),\n",
       " ('weed', 0.262786328792572),\n",
       " ('breed', 0.25418415665626526),\n",
       " ('dream', 0.24939578771591187),\n",
       " ('loyalty', 0.2438662201166153),\n",
       " ('daughter', 0.23810240626335144),\n",
       " ('tree', 0.23642070591449738),\n",
       " ('kid', 0.2338743656873703)]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('seed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more interesting are the results we get from using the `positive` and `negative` keywords. We'll use the \"seed\" example. The positive words contribute positively towards the similarity score; the negative words contribute negatively. When we use \"seed\" as our target word and don't specify a `negative` word, we get a double meaning. But when we add \"weed\" as a `negative` word, the meaning becomes much more about offspring. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seed', 0.7398009300231934),\n",
       " ('responsibility', 0.26197338104248047),\n",
       " ('fetus', 0.25151997804641724),\n",
       " ('child', 0.24744100868701935),\n",
       " ('breddern', 0.23935382068157196),\n",
       " ('loyalty', 0.2368765026330948),\n",
       " ('embrace', 0.2257089465856552),\n",
       " ('yosemite', 0.22085259854793549),\n",
       " ('pallbearer', 0.2204713225364685),\n",
       " ('decomposed', 0.21810504794120789)]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\n",
    "    positive=[model['seed']],\n",
    "    negative=[model['weed']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a couple more things you can do with the word vectors. The first will find the word that doesn't match. The second will find the word most similar to the target word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atlanta'"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(['south', 'east', 'west', 'atlanta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'church'"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar_to_given(\n",
    "    'god', \n",
    "    ['street', 'house', 'baby', 'church', 'party', 'struggle', 'loyalty']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Most Similar Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a helper function so we can look up song titles given the song IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_titles(results):\n",
    "    lookup = lambda x: df_train[\n",
    "        df_train.song_id==int(x)\n",
    "    ].song.values[0]\n",
    "    return [\n",
    "        [\n",
    "            i[0].split('|')[0], \n",
    "            lookup(i[0].split('|')[1]), \n",
    "            i[1]\n",
    "        ] for i in results\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find the top n most similar <i>songs</i> to a target word. When we pass in 'midwest' as our target word, it should be no surprise that Tech N9ne and Nelly have an appearance since both rappers are from and rap about the Midwest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Tech_N9ne', 'Planet Rock 2K (Down South Mix)', 0.24092429876327515],\n",
       " ['Tech_N9ne', 'Strange', 0.24040183424949646],\n",
       " ['Tech_N9ne', 'Planet Rock 2K (Original Version)', 0.2370171993970871],\n",
       " ['Tech_N9ne', 'Strangeulation I', 0.2366243600845337],\n",
       " ['Warren_G', 'Gangsta Love', 0.23494234681129456],\n",
       " ['Tech_N9ne', \"Now It's On\", 0.2246299684047699],\n",
       " ['Tech_N9ne', 'P.R. 2K1', 0.22402459383010864],\n",
       " ['Nelly', 'L.A.', 0.2188049554824829],\n",
       " ['Lil_Wayne', 'Banned From TV', 0.21523958444595337],\n",
       " ['Method_Man', \"Release Yo' Delf\", 0.21435599029064178]]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_titles(\n",
    "    model.docvecs.most_similar([model['midwest']], topn=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not surprising is that when our target word is 'eminem', Eminem and Eminem's band D12 dominate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Eminem', 'Ken Kaniff (Skit)', 0.26885756850242615],\n",
       " ['Eminem', 'Ken Kaniff (Skit)', 0.2669536769390106],\n",
       " ['D12', 'Commercial Break', 0.25570592284202576],\n",
       " ['Eminem', 'The Kiss (Skit)', 0.2381049543619156],\n",
       " ['D12', 'Steve Berman (Skit)', 0.2308967411518097],\n",
       " ['D12', 'Words Are Weapons', 0.2302926629781723],\n",
       " ['D12', 'American Psycho II', 0.2270514965057373],\n",
       " ['Eminem', \"Drop the Bomb On 'Em\", 0.2205711007118225],\n",
       " ['Eminem', 'My Name Is', 0.21902979910373688],\n",
       " ['Fat_Joe', 'My Fofo', 0.21522411704063416]]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_titles(\n",
    "    model.docvecs.most_similar([model['eminem']], topn=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next one is probably the most fasinating result. When our target word is \"church\", we get results that clearly have an element of \"church\" in them. Just look at the first two results, The Game's Hallelujah and Ice Cube's When I Get to Heaven. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The_Game', 'Hallelujah', 0.2645237445831299],\n",
       " ['Ice_Cube', 'When I Get to Heaven', 0.2541915774345398],\n",
       " ['Yelawolf', 'The Last Song', 0.21845132112503052],\n",
       " ['Scarface', 'Crack', 0.21555303037166595],\n",
       " ['Lauryn_Hill', 'Interlude 5', 0.21299612522125244],\n",
       " ['KRS-One', \"Ain't Ready\", 0.21055129170417786],\n",
       " ['Missy_Elliott', 'Intro', 0.20643793046474457],\n",
       " ['Talib_Kweli', \"Give 'Em Hell\", 0.19774499535560608],\n",
       " ['Tech_N9ne', 'Sad Circus', 0.1967805027961731],\n",
       " ['Tech_N9ne', 'Show Me a God', 0.19402025640010834]]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_titles(\n",
    "    model.docvecs.most_similar([model['church']], topn=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find songs that are semantically similar to each other by looking up a word vector using the document tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Eminem', 'The Way I Am', 0.9999999403953552],\n",
       " ['Eminem', 'The Way I Am (Danny Lohner Remix)', 0.9738667011260986],\n",
       " ['Eminem', 'The Way I Am', 0.9721589088439941],\n",
       " ['Nas', 'Album Intro', 0.5593523383140564],\n",
       " ['Immortal_Technique', 'Understand Why', 0.4280475974082947],\n",
       " ['Nate_Dogg', \"I Don't Wanna Hurt No More\", 0.41760876774787903],\n",
       " ['LL_Cool_J', 'Skit', 0.41418007016181946],\n",
       " ['Big_L', 'Platinum Plus', 0.4120897054672241],\n",
       " ['Big_L', 'Platinum Plus', 0.4068526029586792],\n",
       " ['Gang_Starr', 'My Advice 2 You', 0.40361344814300537]]"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_titles(\n",
    "    model.docvecs.most_similar([model.docvecs['Eminem|3006']], topn=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of these are duplicates due to the lyrics site that powers Cypher being community generated, but you get the idea. We can also detect which documents do not belong in a list of documents by using the `doesnt_match` method. Here, we choose which song doesn't match among Eminem's The Way I Am, The Game's Hallelujah and Ice Cube's When I Get to Heaven. The result seems sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Eminem|3006'"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.doesnt_match(['Eminem|3006', 'The_Game|10060', 'Ice_Cube|644'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infering Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll use our test data to see which songs are the most semantically similar to each other. First, let's load our test data then choose a song as input into the `infer_vector` method. We'll choose Eminem's Just the Two of Us, which is `song_id` 1644."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'lyrics_test.csv'\n",
    "test_sentences = Sentences(filename=filename, column='word')\n",
    "df = pd.read_csv(filename)\n",
    "lyrics_str = df[df.song_id==1644].word.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll feed the lyrics into `infer_vector` to return a vector representation of the song. We'll then input that vector representation into `model.docvecs.most_similar` to return back the 10 most similar songs. You can look all the songs up using the ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Gang_Starr', 'Daily Operation (Intro)', 0.6264939308166504],\n",
       " ['Gang_Starr', 'My Advice 2 You', 0.6025712490081787],\n",
       " ['De_La_Soul', 'The Dawn Brings Smoke', 0.6022455096244812],\n",
       " ['De_La_Soul', 'Stickabush', 0.574888288974762],\n",
       " ['Fabolous', 'Niggas Know', 0.5698577761650085],\n",
       " ['Too_$hort', \"Can't Stay Away (Outro)\", 0.5679160356521606],\n",
       " ['Immortal_Technique', 'Apocrypha (Interlude)', 0.5677438378334045],\n",
       " ['Twista', 'Wide Open', 0.5660445690155029],\n",
       " ['Big_Daddy_Kane', 'Looks Like A Job For...', 0.5610144734382629],\n",
       " ['Method_Man', 'Dooney Boy (Skit)', 0.5602116584777832]]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sample = sentence.words\n",
    "ivec = model.infer_vector(\n",
    "    doc_words=lyrics_str, \n",
    "    steps=500, \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "print_titles(\n",
    "    model.docvecs.most_similar([ivec], topn=10) \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool!\n",
    "\n",
    "As you can see, Doc2Vec provides a lot of insight. But we didn't even get to the good stuff: using this data to train machine learning models. Doc2Vec produces `numpy` feature vectors which allow us to use them as training data for machine learning algorithms. In the next post, we'll do just this. I'll train a model that predicts an artist given a song's lyrics. To do this, we'll employ two machine learning classification algorithms, Naive Bayes and Support Vector Machines. See you next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Lyric Attribution using Naive Bayes and Support Vector Machines <br/>\n",
    "• Predicting A Song's Genre Given Its Lyrics <br/>\n",
    "• Topic Modeling with Latent Dirichlet Allocation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
